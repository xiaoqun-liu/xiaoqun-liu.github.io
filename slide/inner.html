<!DOCTYPE html>
<html>

<!-- <body>
    <h1 style="color: grey; font-size: 36px; text-align: center; margin-top: 50px;">Seeking a potential advisor as I
        plan to apply for a Ph.D. program</h1>
</body> -->

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>LXQ homepage</title>

    <!-- favicon -->
    <!-- <link href="favicon.png" rel=icon> -->

    <!-- web-fonts -->
    <link href="https://fonts.googleapis.com/css?family=Hind:300,400,500,600,700" rel="stylesheet">

    <!-- font-awesome -->
    <link href="../css/font-awesome.min.css" rel="stylesheet">

    <!-- Bootstrap -->
    <link href="../css/bootstrap.min.css" rel="stylesheet">

    <!-- Style CSS -->
    <link href="../css/style.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
</head>

<body id="page-top" data-spy="scroll" data-target=".navbar">
    <!-- <header class="page-header sticky-top">
        <div class="container">
            <a href="index.html" class="btn btn-header">Homepage</a>
        </div>
        <div class="container">
        </div>
    </header> -->


    <div id="main-wrapper">
        <!-- Page Preloader -->
        <div id="preloader">
            <div id="status">
                <div class="status-mes"></div>
            </div>
        </div>

        
        <div class="columns-block container">
            <div class="right-col-block blocks">
                <div class="theiaStickySidebar">
                    <section class="section-wrapper section-experience gray-bg" id="research">
                        <div class="container-fluid">
                            <div class="row">
                                <div class="col-md-12">
                                    <div class="section-title">
                                        <h2>Dig Deep into inner LLM</h2>
                                    </div>
                                </div>
                            </div>
                            <!--.row-->
                            <div class="row">
                                <div class="col-md-12">
                                    <div id="research_i">
                                        <div class="content-item">
                                            <small>5/2024 - present</small>
                                            <h3>Periodicity of Large Language Models <a href="../404.html">Details</a></h3>
                                            <h4>Research Assistant</h4>
                                            <img src="../img/backchannel.png" alt="" width=100% height=auto><br>
                                            Huamn language exhibits a degree of periodicity in behavior and outputs,
                                            which
                                            can be analyzed through spectral analysis techniques to gain insights into
                                            the
                                            underlying patterns and dynamics of the LLMs. The potential aspects are
                                            water
                                            marks, eye tracking optimization and so on.
                                            <p>Under the guidance of Professor Yang Xu, SUSTech, Shen Zhen, Guang Dong,
                                                China</p>
                                        </div>

                                        <div class="content-item">
                                            <small>5/2024 - present</small>
                                            <h3>Compression of Large Language Models <a href="../404.html">Details</a></h3>
                                            <h4>Research Assistant</h4>
                                            As the scale of large language models (LLMs) continues to grow, the demand
                                            for efficient model compression and pruning techniques becomes increasingly
                                            critical. However, current approaches either require extensive retraining or
                                            involve post-training quantization and pruning, both of which are
                                            computationally expensive and consume substantial resources. To trackle this
                                            issue, we present BaR (Block-aware Reservation after Post-Training Pruning),
                                            a new and effective approach to prune LLMs. Experiments undergo.
                                            <p>Work with my research partners in SUSTech and HKU.</p>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <!--.row-->
                        </div>
                        <!-- .container-fluid -->

                    </section>
                </div>
            </div>
        </div>
    </div>
    <!-- #main-wrapper -->

    <!-- jquery -->
    <script src="../js/jquery-2.1.4.min.js"></script>

    <!-- Bootstrap -->
    <script src="../js/bootstrap.min.js"></script>
    <script src="../js/theia-sticky-sidebar.js"></script>
    <script src="../js/scripts.js"></script>
</body>

</html>