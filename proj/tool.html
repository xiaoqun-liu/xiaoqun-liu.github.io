<!DOCTYPE html>
<html>

<!-- <body>
    <h1 style="color: grey; font-size: 36px; text-align: center; margin-top: 50px;">Seeking a potential advisor as I
        plan to apply for a Ph.D. program</h1>
</body> -->

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>LLM Alignment</title>

    <!-- favicon -->
    <!-- <link href="favicon.png" rel=icon> -->

    <!-- web-fonts -->
    <link href="https://fonts.googleapis.com/css?family=Hind:300,400,500,600,700" rel="stylesheet">

    <!-- font-awesome -->
    <link href="../css/font-awesome.min.css" rel="stylesheet">

    <!-- Bootstrap -->
    <link href="../css/bootstrap.min.css" rel="stylesheet">

    <!-- Style CSS -->
    <link href="../css/style.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
</head>

<body id="page-top" data-spy="scroll" data-target=".navbar">
    <header class="page-header sticky-top">
        <div class="container">
            <a href="../index.html" class="btn btn-header">Homepage</a>
        </div>
        <div class="container">

        </div>

    </header>




    <div id="main-wrapper">
        <!-- Page Preloader -->
        <div id="preloader">
            <div id="status">
                <div class="status-mes"></div>
            </div>
        </div>
        <div class="columns-block container">

            <!-- left -->
            <div class="left-col-block blocks" id="home">
                <header class="header theiaStickySidebar">
                    <div class="profile-img">
                        <img src="../img/tool.png" class="img-responsive" alt="" />
                    </div>
                    <div class="content">
                        <h1>ARE LANGAUGE MODELS OVERSCALED? SLIMMING
                            LLMS AS BETTER TOOL LEARNING AGENTS</h1>
                        <h3>Xiaoqun Liu, Zhaohan Xi, SUNY Binghamton, Chenyu You, SUNY Stony Brook</h3>
                        
                    </div>

                </header>
                <!-- .header-->
            </div>

            <!-- right -->
            <div class="right-col-block blocks">
                <div class="theiaStickySidebar">
                    <!-- project -->
                    <section class="section-wrapper section-project gray-bg">
                        <div class="container-fluid" id="pro">
                            <div class="row">
                                <div class="col-md-12">
                                    <h2>Explore the inner comminication features.</h2>
                                    <img src="../img/tooll.png" class="img-responsive" alt="" />
                                    <div class="section-title">
                                       
                                    </div>
                                </div>
                            </div>
                        </div>
                        <!-- .container-fluid -->
                        <p>This work addresses the development of large language models (LLMs) with a focus on tool learning, which enhances LLM capabilities by enabling dynamic interactions with external systems rather than relying solely on internal parameters. We pose two key questions:</p>
    
                        <ul>
                            <li><strong>Q₁:</strong> Are large-scale parameters necessary when developing tool agents?</li>
                            <li><strong>Q₂:</strong> Is there an optimal model scale that maintains robust tool learning capabilities while significantly reducing computational resource consumption?</li>
                        </ul>
                    
                        <p>Our initial empirical observations indicate that smaller LLM variants (e.g., Llama-2-7b) demonstrate tool learning performance comparable to their larger counterparts (e.g., Llama-2-70b), while significantly smaller models (e.g., GPT-neo 125m) exhibit markedly reduced capabilities. This suggests a trade-off between the scale of an LLM and its tool learning capacity.</p>
                    
                        <p>To explore this trade-off, we developed <strong>SLM</strong>, an openly accessible framework designed to "slim" LLMs while preserving their tool learning capabilities. SLM integrates various LLM compression methods into three principal categories: pruning, quantization, and distillation. For each method, SLM conducts a series of model compressions and provides visual feedback on tool learning performance and computational resource utilization.</p>
                    
                        <p>SLM focuses on evaluating tool learning metrics across four distinct scenarios: general-purpose tool selection, domain-specific tool selection, multi-tool selection, and interactive task planning with environmental feedback. We demonstrate the application of SLM with leading LLMs, such as Llama-3.1, showcasing how it effectively maintains tool learning performance while slimming LLMs.</p>
                    
                        <h2>Structure of the Paper</h2>
                        <p>In summary, this paper is structured into three phases:</p>
                        
                        <ul>
                            <li><strong>Experimental Findings:</strong> We explore the potential for LLMs to perform tool learning at reduced scales, revealing a specific trade-off between tool learning capabilities and the scale of LLMs.</li>
                            <li><strong>Methodology:</strong> We design, develop, and deploy SLM, an innovative framework for slimming LLMs to preserve their tool learning capabilities.</li>
                            <li><strong>Comprehensive Studies:</strong> We apply SLM to slim leading open-source LLMs, evaluating tool understanding and interactive execution capabilities.</li>
                        </ul>
                    
                        <p><strong>Disclaimer:</strong> This work does not diminish the importance of developing larger-scale foundation models. Instead, it focuses on the niche of "LLM-as-tool-learning-agent" and explores how to efficiently create tool agents with reduced parameter counts, termed "slimming LLMs" in our study. This approach could be particularly beneficial for industries that integrate LLMs with real-world tools in specific scenarios.</p>
                    </section>

                    <footer class="footer">
                        <div class="copyright-section">
                            <div class="container-fluid">
                                <div class="row">
                                    <div class="col-md-12">
                                        <div class="copytext">&copy; Resume. All rights reserved </div>
                                    </div>
                                </div>
                                <!--.row-->
                            </div>
                            <!-- .container-fluid -->
                        </div>
                        <!-- .copyright-section -->
                    </footer>
                    <!-- .footer -->
                </div>
                <!-- Sticky -->
            </div>
            <!-- .right-col-block -->
        </div>
        <!-- .columns-block -->
    </div>
    <!-- #main-wrapper -->

    <!-- jquery -->
    <script src="../js/jquery-2.1.4.min.js"></script>

    <!-- Bootstrap -->
    <script src="../js/bootstrap.min.js"></script>
    <script src="../js/theia-sticky-sidebar.js"></script>
    <script src="../js/scripts.js"></script>
</body>

</html>